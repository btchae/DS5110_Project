{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS 5110 Final Project - Hospital Mortality\n",
    "### By: Elena Tsvetskova, Brian Chae, Ryan Viti (rrv7eb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------+---+-----+----------------+---------+------+------+----------------+------+-------------+------------+----------------+------+------------------+-------------------+---------------------+----------+---------------+----------------+-----------------+-----------------+-----------------+----------------+----------+---------------+-----------+-----------------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+-----------+-----------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+--------------+--------------+----------------+----------------+-----------------------------+------------------------+----+---------+-----------------+---------------+-----------------+--------+--------+---------------------------+--------------------+-------------------+----+--------------+\n",
      "|encounter_id|patient_id|hospital_id|age|  bmi|elective_surgery|ethnicity|gender|height|icu_admit_source|icu_id|icu_stay_type|    icu_type|pre_icu_los_days|weight|apache_2_diagnosis|apache_3j_diagnosis|apache_post_operative|arf_apache|gcs_eyes_apache|gcs_motor_apache|gcs_unable_apache|gcs_verbal_apache|heart_rate_apache|intubated_apache|map_apache|resprate_apache|temp_apache|ventilated_apache|d1_diasbp_max|d1_diasbp_min|d1_diasbp_noninvasive_max|d1_diasbp_noninvasive_min|d1_heartrate_max|d1_heartrate_min|d1_mbp_max|d1_mbp_min|d1_mbp_noninvasive_max|d1_mbp_noninvasive_min|d1_resprate_max|d1_resprate_min|d1_spo2_max|d1_spo2_min|d1_sysbp_max|d1_sysbp_min|d1_sysbp_noninvasive_max|d1_sysbp_noninvasive_min|d1_temp_max|d1_temp_min|h1_diasbp_max|h1_diasbp_min|h1_diasbp_noninvasive_max|h1_diasbp_noninvasive_min|h1_heartrate_max|h1_heartrate_min|h1_mbp_max|h1_mbp_min|h1_mbp_noninvasive_max|h1_mbp_noninvasive_min|h1_resprate_max|h1_resprate_min|h1_spo2_max|h1_spo2_min|h1_sysbp_max|h1_sysbp_min|h1_sysbp_noninvasive_max|h1_sysbp_noninvasive_min|d1_glucose_max|d1_glucose_min|d1_potassium_max|d1_potassium_min|apache_4a_hospital_death_prob|apache_4a_icu_death_prob|aids|cirrhosis|diabetes_mellitus|hepatic_failure|immunosuppression|leukemia|lymphoma|solid_tumor_with_metastasis|apache_3j_bodysystem|apache_2_bodysystem|_c83|hospital_death|\n",
      "+------------+----------+-----------+---+-----+----------------+---------+------+------+----------------+------+-------------+------------+----------------+------+------------------+-------------------+---------------------+----------+---------------+----------------+-----------------+-----------------+-----------------+----------------+----------+---------------+-----------+-----------------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+-----------+-----------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+--------------+--------------+----------------+----------------+-----------------------------+------------------------+----+---------+-----------------+---------------+-----------------+--------+--------+---------------------------+--------------------+-------------------+----+--------------+\n",
      "|       66154|     25312|        118| 68|22.73|               0|Caucasian|     M| 180.3|           Floor|    92|        admit|       CTICU|     0.541666667|  73.9|               113|             502.01|                    0|         0|              3|               6|                0|                4|              118|               0|        40|           36.0|       39.3|                0|           68|           37|                       68|                       37|             119|              72|        89|        46|                    89|                    46|             34|             10|        100|         74|         131|          73|                     131|                    73.0|       39.9|       37.2|           68|           63|                       68|                       63|             119|             108|        86|        85|                    86|                    85|             26|             18|        100|         74|         131|         115|                     131|                     115|           168|           109|             4.0|             3.4|                          0.1|                    0.05|   0|        0|                1|              0|                0|       0|       0|                          0|              Sepsis|     Cardiovascular|null|             0|\n",
      "|      114252|     59342|         81| 77|27.42|               0|Caucasian|     F| 160.0|           Floor|    90|        admit|Med-Surg ICU|     0.927777778|  70.2|               108|             203.01|                    0|         0|              1|               3|                0|                1|              120|               0|        46|           33.0|       35.1|                1|           95|           31|                       95|                       31|             118|              72|       120|        38|                   120|                    38|             32|             12|        100|         70|         159|          67|                     159|                    67.0|       36.3|       35.1|           61|           48|                       61|                       48|             114|             100|        85|        57|                    85|                    57|             31|             28|         95|         70|          95|          71|                      95|                      71|           145|           128|             4.2|             3.8|                         0.47|                    0.29|   0|        0|                1|              0|                0|       0|       0|                          0|         Respiratory|        Respiratory|null|             0|\n",
      "+------------+----------+-----------+---+-----+----------------+---------+------+------+----------------+------+-------------+------------+----------------+------+------------------+-------------------+---------------------+----------+---------------+----------------+-----------------+-----------------+-----------------+----------------+----------+---------------+-----------+-----------------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+-----------+-----------+-------------+-------------+-------------------------+-------------------------+----------------+----------------+----------+----------+----------------------+----------------------+---------------+---------------+-----------+-----------+------------+------------+------------------------+------------------------+--------------+--------------+----------------+----------------+-----------------------------+------------------------+----+---------+-----------------+---------------+-----------------+--------+--------+---------------------------+--------------------+-------------------+----+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Team 3 Final Project\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "filename = \"hospital_mortality.csv\"\n",
    "\n",
    "data = spark.read.csv(filename, inferSchema=True, header=True)\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- encounter_id: integer (nullable = true)\n",
      " |-- patient_id: integer (nullable = true)\n",
      " |-- hospital_id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- elective_surgery: integer (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- icu_admit_source: string (nullable = true)\n",
      " |-- icu_id: integer (nullable = true)\n",
      " |-- icu_stay_type: string (nullable = true)\n",
      " |-- icu_type: string (nullable = true)\n",
      " |-- pre_icu_los_days: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- apache_2_diagnosis: integer (nullable = true)\n",
      " |-- apache_3j_diagnosis: double (nullable = true)\n",
      " |-- apache_post_operative: integer (nullable = true)\n",
      " |-- arf_apache: integer (nullable = true)\n",
      " |-- gcs_eyes_apache: integer (nullable = true)\n",
      " |-- gcs_motor_apache: integer (nullable = true)\n",
      " |-- gcs_unable_apache: integer (nullable = true)\n",
      " |-- gcs_verbal_apache: integer (nullable = true)\n",
      " |-- heart_rate_apache: integer (nullable = true)\n",
      " |-- intubated_apache: integer (nullable = true)\n",
      " |-- map_apache: integer (nullable = true)\n",
      " |-- resprate_apache: double (nullable = true)\n",
      " |-- temp_apache: double (nullable = true)\n",
      " |-- ventilated_apache: integer (nullable = true)\n",
      " |-- d1_diasbp_max: integer (nullable = true)\n",
      " |-- d1_diasbp_min: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_heartrate_max: integer (nullable = true)\n",
      " |-- d1_heartrate_min: integer (nullable = true)\n",
      " |-- d1_mbp_max: integer (nullable = true)\n",
      " |-- d1_mbp_min: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_resprate_max: integer (nullable = true)\n",
      " |-- d1_resprate_min: integer (nullable = true)\n",
      " |-- d1_spo2_max: integer (nullable = true)\n",
      " |-- d1_spo2_min: integer (nullable = true)\n",
      " |-- d1_sysbp_max: integer (nullable = true)\n",
      " |-- d1_sysbp_min: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_min: double (nullable = true)\n",
      " |-- d1_temp_max: double (nullable = true)\n",
      " |-- d1_temp_min: double (nullable = true)\n",
      " |-- h1_diasbp_max: integer (nullable = true)\n",
      " |-- h1_diasbp_min: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_heartrate_max: integer (nullable = true)\n",
      " |-- h1_heartrate_min: integer (nullable = true)\n",
      " |-- h1_mbp_max: integer (nullable = true)\n",
      " |-- h1_mbp_min: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_resprate_max: integer (nullable = true)\n",
      " |-- h1_resprate_min: integer (nullable = true)\n",
      " |-- h1_spo2_max: integer (nullable = true)\n",
      " |-- h1_spo2_min: integer (nullable = true)\n",
      " |-- h1_sysbp_max: integer (nullable = true)\n",
      " |-- h1_sysbp_min: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_glucose_max: integer (nullable = true)\n",
      " |-- d1_glucose_min: integer (nullable = true)\n",
      " |-- d1_potassium_max: double (nullable = true)\n",
      " |-- d1_potassium_min: double (nullable = true)\n",
      " |-- apache_4a_hospital_death_prob: double (nullable = true)\n",
      " |-- apache_4a_icu_death_prob: double (nullable = true)\n",
      " |-- aids: integer (nullable = true)\n",
      " |-- cirrhosis: integer (nullable = true)\n",
      " |-- diabetes_mellitus: integer (nullable = true)\n",
      " |-- hepatic_failure: integer (nullable = true)\n",
      " |-- immunosuppression: integer (nullable = true)\n",
      " |-- leukemia: integer (nullable = true)\n",
      " |-- lymphoma: integer (nullable = true)\n",
      " |-- solid_tumor_with_metastasis: integer (nullable = true)\n",
      " |-- apache_3j_bodysystem: string (nullable = true)\n",
      " |-- apache_2_bodysystem: string (nullable = true)\n",
      " |-- _c83: string (nullable = true)\n",
      " |-- hospital_death: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- elective_surgery: integer (nullable = true)\n",
      " |-- ethnicity: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- icu_admit_source: string (nullable = true)\n",
      " |-- icu_type: string (nullable = true)\n",
      " |-- pre_icu_los_days: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- apache_2_diagnosis: integer (nullable = true)\n",
      " |-- apache_3j_diagnosis: double (nullable = true)\n",
      " |-- apache_post_operative: integer (nullable = true)\n",
      " |-- arf_apache: integer (nullable = true)\n",
      " |-- gcs_eyes_apache: integer (nullable = true)\n",
      " |-- gcs_motor_apache: integer (nullable = true)\n",
      " |-- gcs_unable_apache: integer (nullable = true)\n",
      " |-- gcs_verbal_apache: integer (nullable = true)\n",
      " |-- heart_rate_apache: integer (nullable = true)\n",
      " |-- intubated_apache: integer (nullable = true)\n",
      " |-- map_apache: integer (nullable = true)\n",
      " |-- resprate_apache: double (nullable = true)\n",
      " |-- temp_apache: double (nullable = true)\n",
      " |-- ventilated_apache: integer (nullable = true)\n",
      " |-- d1_diasbp_max: integer (nullable = true)\n",
      " |-- d1_diasbp_min: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_heartrate_max: integer (nullable = true)\n",
      " |-- d1_heartrate_min: integer (nullable = true)\n",
      " |-- d1_mbp_max: integer (nullable = true)\n",
      " |-- d1_mbp_min: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_resprate_max: integer (nullable = true)\n",
      " |-- d1_resprate_min: integer (nullable = true)\n",
      " |-- d1_spo2_max: integer (nullable = true)\n",
      " |-- d1_spo2_min: integer (nullable = true)\n",
      " |-- d1_sysbp_max: integer (nullable = true)\n",
      " |-- d1_sysbp_min: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_min: double (nullable = true)\n",
      " |-- d1_temp_max: double (nullable = true)\n",
      " |-- d1_temp_min: double (nullable = true)\n",
      " |-- h1_diasbp_max: integer (nullable = true)\n",
      " |-- h1_diasbp_min: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_heartrate_max: integer (nullable = true)\n",
      " |-- h1_heartrate_min: integer (nullable = true)\n",
      " |-- h1_mbp_max: integer (nullable = true)\n",
      " |-- h1_mbp_min: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_resprate_max: integer (nullable = true)\n",
      " |-- h1_resprate_min: integer (nullable = true)\n",
      " |-- h1_spo2_max: integer (nullable = true)\n",
      " |-- h1_spo2_min: integer (nullable = true)\n",
      " |-- h1_sysbp_max: integer (nullable = true)\n",
      " |-- h1_sysbp_min: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_glucose_max: integer (nullable = true)\n",
      " |-- d1_glucose_min: integer (nullable = true)\n",
      " |-- d1_potassium_max: double (nullable = true)\n",
      " |-- d1_potassium_min: double (nullable = true)\n",
      " |-- apache_4a_hospital_death_prob: double (nullable = true)\n",
      " |-- apache_4a_icu_death_prob: double (nullable = true)\n",
      " |-- aids: integer (nullable = true)\n",
      " |-- cirrhosis: integer (nullable = true)\n",
      " |-- diabetes_mellitus: integer (nullable = true)\n",
      " |-- hepatic_failure: integer (nullable = true)\n",
      " |-- immunosuppression: integer (nullable = true)\n",
      " |-- leukemia: integer (nullable = true)\n",
      " |-- lymphoma: integer (nullable = true)\n",
      " |-- solid_tumor_with_metastasis: integer (nullable = true)\n",
      " |-- apache_3j_bodysystem: string (nullable = true)\n",
      " |-- apache_2_bodysystem: string (nullable = true)\n",
      " |-- hospital_death: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty columns and unique identifiers unnecessary for analysis\n",
    "\n",
    "cleaned_data = data.drop(\"_c83\", \"encounter_id\", \"patient_id\", \"hospital_id\", \"icu_id\", \"icu_stay_type\")\n",
    "cleaned_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=68, bmi=22.73, elective_surgery=0, ethnicity='Caucasian', gender='M', height=180.3, icu_admit_source='Floor', icu_type='CTICU', pre_icu_los_days=0.541666667, weight=73.9, apache_2_diagnosis=113, apache_3j_diagnosis=502.01, apache_post_operative=0, arf_apache=0, gcs_eyes_apache=3, gcs_motor_apache=6, gcs_unable_apache=0, gcs_verbal_apache=4, heart_rate_apache=118, intubated_apache=0, map_apache=40, resprate_apache=36.0, temp_apache=39.3, ventilated_apache=0, d1_diasbp_max=68, d1_diasbp_min=37, d1_diasbp_noninvasive_max=68, d1_diasbp_noninvasive_min=37, d1_heartrate_max=119, d1_heartrate_min=72, d1_mbp_max=89, d1_mbp_min=46, d1_mbp_noninvasive_max=89, d1_mbp_noninvasive_min=46, d1_resprate_max=34, d1_resprate_min=10, d1_spo2_max=100, d1_spo2_min=74, d1_sysbp_max=131, d1_sysbp_min=73, d1_sysbp_noninvasive_max=131, d1_sysbp_noninvasive_min=73.0, d1_temp_max=39.9, d1_temp_min=37.2, h1_diasbp_max=68, h1_diasbp_min=63, h1_diasbp_noninvasive_max=68, h1_diasbp_noninvasive_min=63, h1_heartrate_max=119, h1_heartrate_min=108, h1_mbp_max=86, h1_mbp_min=85, h1_mbp_noninvasive_max=86, h1_mbp_noninvasive_min=85, h1_resprate_max=26, h1_resprate_min=18, h1_spo2_max=100, h1_spo2_min=74, h1_sysbp_max=131, h1_sysbp_min=115, h1_sysbp_noninvasive_max=131, h1_sysbp_noninvasive_min=115, d1_glucose_max=168, d1_glucose_min=109, d1_potassium_max=4.0, d1_potassium_min=3.4, apache_4a_hospital_death_prob=0.1, apache_4a_icu_death_prob=0.05, aids=0, cirrhosis=0, diabetes_mellitus=1, hepatic_failure=0, immunosuppression=0, leukemia=0, lymphoma=0, solid_tumor_with_metastasis=0, apache_3j_bodysystem='Sepsis', apache_2_bodysystem='Cardiovascular', hospital_death=0, ethnicityIndex=0.0, genderIndex=0.0, icu_admit_sourceIndex=2.0, icu_typeIndex=7.0, apache_3j_bodysystemIndex=2.0, apache_2_bodysystemIndex=0.0),\n",
       " Row(age=77, bmi=27.42, elective_surgery=0, ethnicity='Caucasian', gender='F', height=160.0, icu_admit_source='Floor', icu_type='Med-Surg ICU', pre_icu_los_days=0.927777778, weight=70.2, apache_2_diagnosis=108, apache_3j_diagnosis=203.01, apache_post_operative=0, arf_apache=0, gcs_eyes_apache=1, gcs_motor_apache=3, gcs_unable_apache=0, gcs_verbal_apache=1, heart_rate_apache=120, intubated_apache=0, map_apache=46, resprate_apache=33.0, temp_apache=35.1, ventilated_apache=1, d1_diasbp_max=95, d1_diasbp_min=31, d1_diasbp_noninvasive_max=95, d1_diasbp_noninvasive_min=31, d1_heartrate_max=118, d1_heartrate_min=72, d1_mbp_max=120, d1_mbp_min=38, d1_mbp_noninvasive_max=120, d1_mbp_noninvasive_min=38, d1_resprate_max=32, d1_resprate_min=12, d1_spo2_max=100, d1_spo2_min=70, d1_sysbp_max=159, d1_sysbp_min=67, d1_sysbp_noninvasive_max=159, d1_sysbp_noninvasive_min=67.0, d1_temp_max=36.3, d1_temp_min=35.1, h1_diasbp_max=61, h1_diasbp_min=48, h1_diasbp_noninvasive_max=61, h1_diasbp_noninvasive_min=48, h1_heartrate_max=114, h1_heartrate_min=100, h1_mbp_max=85, h1_mbp_min=57, h1_mbp_noninvasive_max=85, h1_mbp_noninvasive_min=57, h1_resprate_max=31, h1_resprate_min=28, h1_spo2_max=95, h1_spo2_min=70, h1_sysbp_max=95, h1_sysbp_min=71, h1_sysbp_noninvasive_max=95, h1_sysbp_noninvasive_min=71, d1_glucose_max=145, d1_glucose_min=128, d1_potassium_max=4.2, d1_potassium_min=3.8, apache_4a_hospital_death_prob=0.47, apache_4a_icu_death_prob=0.29, aids=0, cirrhosis=0, diabetes_mellitus=1, hepatic_failure=0, immunosuppression=0, leukemia=0, lymphoma=0, solid_tumor_with_metastasis=0, apache_3j_bodysystem='Respiratory', apache_2_bodysystem='Respiratory', hospital_death=0, ethnicityIndex=0.0, genderIndex=1.0, icu_admit_sourceIndex=2.0, icu_typeIndex=0.0, apache_3j_bodysystemIndex=3.0, apache_2_bodysystemIndex=2.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pyspark modules\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "#create a list of the columns that are string typed\n",
    "categoricalColumns = [item[0] for item in cleaned_data.dtypes if item[1].startswith('string') ]\n",
    "\n",
    "#define a list of stages in your pipeline. The string indexer will be one stage\n",
    "stages = []\n",
    "\n",
    "#iterate through all categorical values\n",
    "for categoricalCol in categoricalColumns:\n",
    "    #create a string indexer for those categorical values and assign a new name including the word 'Index'\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "\n",
    "    #append the string Indexer to our list of stages\n",
    "    stages += [stringIndexer]\n",
    "\n",
    "#Create the pipeline. Assign the satges list to the pipeline key word stages\n",
    "pipeline = Pipeline(stages = stages)\n",
    "#fit the pipeline to our dataframe\n",
    "pipelineModel = pipeline.fit(cleaned_data)\n",
    "#transform the dataframe\n",
    "df= pipelineModel.transform(cleaned_data)\n",
    "df.take(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- elective_surgery: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- pre_icu_los_days: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- apache_2_diagnosis: integer (nullable = true)\n",
      " |-- apache_3j_diagnosis: double (nullable = true)\n",
      " |-- apache_post_operative: integer (nullable = true)\n",
      " |-- arf_apache: integer (nullable = true)\n",
      " |-- gcs_eyes_apache: integer (nullable = true)\n",
      " |-- gcs_motor_apache: integer (nullable = true)\n",
      " |-- gcs_unable_apache: integer (nullable = true)\n",
      " |-- gcs_verbal_apache: integer (nullable = true)\n",
      " |-- heart_rate_apache: integer (nullable = true)\n",
      " |-- intubated_apache: integer (nullable = true)\n",
      " |-- map_apache: integer (nullable = true)\n",
      " |-- resprate_apache: double (nullable = true)\n",
      " |-- temp_apache: double (nullable = true)\n",
      " |-- ventilated_apache: integer (nullable = true)\n",
      " |-- d1_diasbp_max: integer (nullable = true)\n",
      " |-- d1_diasbp_min: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_heartrate_max: integer (nullable = true)\n",
      " |-- d1_heartrate_min: integer (nullable = true)\n",
      " |-- d1_mbp_max: integer (nullable = true)\n",
      " |-- d1_mbp_min: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_resprate_max: integer (nullable = true)\n",
      " |-- d1_resprate_min: integer (nullable = true)\n",
      " |-- d1_spo2_max: integer (nullable = true)\n",
      " |-- d1_spo2_min: integer (nullable = true)\n",
      " |-- d1_sysbp_max: integer (nullable = true)\n",
      " |-- d1_sysbp_min: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- d1_sysbp_noninvasive_min: double (nullable = true)\n",
      " |-- d1_temp_max: double (nullable = true)\n",
      " |-- d1_temp_min: double (nullable = true)\n",
      " |-- h1_diasbp_max: integer (nullable = true)\n",
      " |-- h1_diasbp_min: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_diasbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_heartrate_max: integer (nullable = true)\n",
      " |-- h1_heartrate_min: integer (nullable = true)\n",
      " |-- h1_mbp_max: integer (nullable = true)\n",
      " |-- h1_mbp_min: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_mbp_noninvasive_min: integer (nullable = true)\n",
      " |-- h1_resprate_max: integer (nullable = true)\n",
      " |-- h1_resprate_min: integer (nullable = true)\n",
      " |-- h1_spo2_max: integer (nullable = true)\n",
      " |-- h1_spo2_min: integer (nullable = true)\n",
      " |-- h1_sysbp_max: integer (nullable = true)\n",
      " |-- h1_sysbp_min: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_max: integer (nullable = true)\n",
      " |-- h1_sysbp_noninvasive_min: integer (nullable = true)\n",
      " |-- d1_glucose_max: integer (nullable = true)\n",
      " |-- d1_glucose_min: integer (nullable = true)\n",
      " |-- d1_potassium_max: double (nullable = true)\n",
      " |-- d1_potassium_min: double (nullable = true)\n",
      " |-- apache_4a_hospital_death_prob: double (nullable = true)\n",
      " |-- apache_4a_icu_death_prob: double (nullable = true)\n",
      " |-- aids: integer (nullable = true)\n",
      " |-- cirrhosis: integer (nullable = true)\n",
      " |-- diabetes_mellitus: integer (nullable = true)\n",
      " |-- hepatic_failure: integer (nullable = true)\n",
      " |-- immunosuppression: integer (nullable = true)\n",
      " |-- leukemia: integer (nullable = true)\n",
      " |-- lymphoma: integer (nullable = true)\n",
      " |-- solid_tumor_with_metastasis: integer (nullable = true)\n",
      " |-- hospital_death: integer (nullable = true)\n",
      " |-- ethnicityIndex: double (nullable = false)\n",
      " |-- genderIndex: double (nullable = false)\n",
      " |-- icu_admit_sourceIndex: double (nullable = false)\n",
      " |-- icu_typeIndex: double (nullable = false)\n",
      " |-- apache_3j_bodysystemIndex: double (nullable = false)\n",
      " |-- apache_2_bodysystemIndex: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The number of columns doesn't match.\nOld column names (79): age, bmi, elective_surgery, height, pre_icu_los_days, weight, apache_2_diagnosis, apache_3j_diagnosis, apache_post_operative, arf_apache, gcs_eyes_apache, gcs_motor_apache, gcs_unable_apache, gcs_verbal_apache, heart_rate_apache, intubated_apache, map_apache, resprate_apache, temp_apache, ventilated_apache, d1_diasbp_max, d1_diasbp_min, d1_diasbp_noninvasive_max, d1_diasbp_noninvasive_min, d1_heartrate_max, d1_heartrate_min, d1_mbp_max, d1_mbp_min, d1_mbp_noninvasive_max, d1_mbp_noninvasive_min, d1_resprate_max, d1_resprate_min, d1_spo2_max, d1_spo2_min, d1_sysbp_max, d1_sysbp_min, d1_sysbp_noninvasive_max, d1_sysbp_noninvasive_min, d1_temp_max, d1_temp_min, h1_diasbp_max, h1_diasbp_min, h1_diasbp_noninvasive_max, h1_diasbp_noninvasive_min, h1_heartrate_max, h1_heartrate_min, h1_mbp_max, h1_mbp_min, h1_mbp_noninvasive_max, h1_mbp_noninvasive_min, h1_resprate_max, h1_resprate_min, h1_spo2_max, h1_spo2_min, h1_sysbp_max, h1_sysbp_min, h1_sysbp_noninvasive_max, h1_sysbp_noninvasive_min, d1_glucose_max, d1_glucose_min, d1_potassium_max, d1_potassium_min, apache_4a_hospital_death_prob, apache_4a_icu_death_prob, aids, cirrhosis, diabetes_mellitus, hepatic_failure, immunosuppression, leukemia, lymphoma, solid_tumor_with_metastasis, hospital_death, ethnicityIndex, genderIndex, icu_admit_sourceIndex, icu_typeIndex, apache_3j_bodysystemIndex, apache_2_bodysystemIndex\nNew column names (0): ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1dfd81a78f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ethnicity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gender\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"icu_admit_source\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"icu_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apache_3j_bodysystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"apache_2_bodysystem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2532\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2533\u001b[0m         \"\"\"\n\u001b[0;32m-> 2534\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (79): age, bmi, elective_surgery, height, pre_icu_los_days, weight, apache_2_diagnosis, apache_3j_diagnosis, apache_post_operative, arf_apache, gcs_eyes_apache, gcs_motor_apache, gcs_unable_apache, gcs_verbal_apache, heart_rate_apache, intubated_apache, map_apache, resprate_apache, temp_apache, ventilated_apache, d1_diasbp_max, d1_diasbp_min, d1_diasbp_noninvasive_max, d1_diasbp_noninvasive_min, d1_heartrate_max, d1_heartrate_min, d1_mbp_max, d1_mbp_min, d1_mbp_noninvasive_max, d1_mbp_noninvasive_min, d1_resprate_max, d1_resprate_min, d1_spo2_max, d1_spo2_min, d1_sysbp_max, d1_sysbp_min, d1_sysbp_noninvasive_max, d1_sysbp_noninvasive_min, d1_temp_max, d1_temp_min, h1_diasbp_max, h1_diasbp_min, h1_diasbp_noninvasive_max, h1_diasbp_noninvasive_min, h1_heartrate_max, h1_heartrate_min, h1_mbp_max, h1_mbp_min, h1_mbp_noninvasive_max, h1_mbp_noninvasive_min, h1_resprate_max, h1_resprate_min, h1_spo2_max, h1_spo2_min, h1_sysbp_max, h1_sysbp_min, h1_sysbp_noninvasive_max, h1_sysbp_noninvasive_min, d1_glucose_max, d1_glucose_min, d1_potassium_max, d1_potassium_min, apache_4a_hospital_death_prob, apache_4a_icu_death_prob, aids, cirrhosis, diabetes_mellitus, hepatic_failure, immunosuppression, leukemia, lymphoma, solid_tumor_with_metastasis, hospital_death, ethnicityIndex, genderIndex, icu_admit_sourceIndex, icu_typeIndex, apache_3j_bodysystemIndex, apache_2_bodysystemIndex\nNew column names (0): "
     ]
    }
   ],
   "source": [
    "df_2 = df.drop(\"ethnicity\", \"gender\", \"icu_admit_source\", \"icu_type\", \"apache_3j_bodysystem\", \"apache_2_bodysystem\")\n",
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2000.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 87) (udc-ba34-13c0 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_dec361fd3025:double,bmi:double,elective_surgery_double_VectorAssembler_dec361fd3025:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_dec361fd3025:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_dec361fd3025:double,arf_apache_double_VectorAssembler_dec361fd3025:double,gcs_eyes_apache_double_VectorAssembler_dec361fd3025:double,gcs_motor_apache_double_VectorAssembler_dec361fd3025:double,gcs_unable_apache_double_VectorAssembler_dec361fd3025:double,gcs_verbal_apache_double_VectorAssembler_dec361fd3025:double,heart_rate_apache_double_VectorAssembler_dec361fd3025:double,intubated_apache_double_VectorAssembler_dec361fd3025:double,map_apache_double_VectorAssembler_dec361fd3025:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_dec361fd3025:double,d1_diasbp_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_min_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_heartrate_max_double_VectorAssembler_dec361fd3025:double,d1_heartrate_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_resprate_max_double_VectorAssembler_dec361fd3025:double,d1_resprate_min_double_VectorAssembler_dec361fd3025:double,d1_spo2_max_double_VectorAssembler_dec361fd3025:double,d1_spo2_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_min_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_heartrate_max_double_VectorAssembler_dec361fd3025:double,h1_heartrate_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_resprate_max_double_VectorAssembler_dec361fd3025:double,h1_resprate_min_double_VectorAssembler_dec361fd3025:double,h1_spo2_max_double_VectorAssembler_dec361fd3025:double,h1_spo2_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_glucose_max_double_VectorAssembler_dec361fd3025:double,d1_glucose_min_double_VectorAssembler_dec361fd3025:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_dec361fd3025:double,cirrhosis_double_VectorAssembler_dec361fd3025:double,diabetes_mellitus_double_VectorAssembler_dec361fd3025:double,hepatic_failure_double_VectorAssembler_dec361fd3025:double,immunosuppression_double_VectorAssembler_dec361fd3025:double,leukemia_double_VectorAssembler_dec361fd3025:double,lymphoma_double_VectorAssembler_dec361fd3025:double,solid_tumor_with_metastasis_double_VectorAssembler_dec361fd3025:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:150)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:84)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_dec361fd3025:double,bmi:double,elective_surgery_double_VectorAssembler_dec361fd3025:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_dec361fd3025:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_dec361fd3025:double,arf_apache_double_VectorAssembler_dec361fd3025:double,gcs_eyes_apache_double_VectorAssembler_dec361fd3025:double,gcs_motor_apache_double_VectorAssembler_dec361fd3025:double,gcs_unable_apache_double_VectorAssembler_dec361fd3025:double,gcs_verbal_apache_double_VectorAssembler_dec361fd3025:double,heart_rate_apache_double_VectorAssembler_dec361fd3025:double,intubated_apache_double_VectorAssembler_dec361fd3025:double,map_apache_double_VectorAssembler_dec361fd3025:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_dec361fd3025:double,d1_diasbp_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_min_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_heartrate_max_double_VectorAssembler_dec361fd3025:double,d1_heartrate_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_resprate_max_double_VectorAssembler_dec361fd3025:double,d1_resprate_min_double_VectorAssembler_dec361fd3025:double,d1_spo2_max_double_VectorAssembler_dec361fd3025:double,d1_spo2_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_min_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_heartrate_max_double_VectorAssembler_dec361fd3025:double,h1_heartrate_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_resprate_max_double_VectorAssembler_dec361fd3025:double,h1_resprate_min_double_VectorAssembler_dec361fd3025:double,h1_spo2_max_double_VectorAssembler_dec361fd3025:double,h1_spo2_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_glucose_max_double_VectorAssembler_dec361fd3025:double,d1_glucose_min_double_VectorAssembler_dec361fd3025:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_dec361fd3025:double,cirrhosis_double_VectorAssembler_dec361fd3025:double,diabetes_mellitus_double_VectorAssembler_dec361fd3025:double,hepatic_failure_double_VectorAssembler_dec361fd3025:double,immunosuppression_double_VectorAssembler_dec361fd3025:double,leukemia_double_VectorAssembler_dec361fd3025:double,lymphoma_double_VectorAssembler_dec361fd3025:double,solid_tumor_with_metastasis_double_VectorAssembler_dec361fd3025:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:150)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-42ac86495d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mwithMean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwithStd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m ).fit(output)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# when we transform the dataframe, the old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2000.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 87) (udc-ba34-13c0 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_dec361fd3025:double,bmi:double,elective_surgery_double_VectorAssembler_dec361fd3025:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_dec361fd3025:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_dec361fd3025:double,arf_apache_double_VectorAssembler_dec361fd3025:double,gcs_eyes_apache_double_VectorAssembler_dec361fd3025:double,gcs_motor_apache_double_VectorAssembler_dec361fd3025:double,gcs_unable_apache_double_VectorAssembler_dec361fd3025:double,gcs_verbal_apache_double_VectorAssembler_dec361fd3025:double,heart_rate_apache_double_VectorAssembler_dec361fd3025:double,intubated_apache_double_VectorAssembler_dec361fd3025:double,map_apache_double_VectorAssembler_dec361fd3025:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_dec361fd3025:double,d1_diasbp_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_min_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_heartrate_max_double_VectorAssembler_dec361fd3025:double,d1_heartrate_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_resprate_max_double_VectorAssembler_dec361fd3025:double,d1_resprate_min_double_VectorAssembler_dec361fd3025:double,d1_spo2_max_double_VectorAssembler_dec361fd3025:double,d1_spo2_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_min_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_heartrate_max_double_VectorAssembler_dec361fd3025:double,h1_heartrate_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_resprate_max_double_VectorAssembler_dec361fd3025:double,h1_resprate_min_double_VectorAssembler_dec361fd3025:double,h1_spo2_max_double_VectorAssembler_dec361fd3025:double,h1_spo2_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_glucose_max_double_VectorAssembler_dec361fd3025:double,d1_glucose_min_double_VectorAssembler_dec361fd3025:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_dec361fd3025:double,cirrhosis_double_VectorAssembler_dec361fd3025:double,diabetes_mellitus_double_VectorAssembler_dec361fd3025:double,hepatic_failure_double_VectorAssembler_dec361fd3025:double,immunosuppression_double_VectorAssembler_dec361fd3025:double,leukemia_double_VectorAssembler_dec361fd3025:double,lymphoma_double_VectorAssembler_dec361fd3025:double,solid_tumor_with_metastasis_double_VectorAssembler_dec361fd3025:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:150)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2729)\n\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2736)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\n\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:84)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_dec361fd3025:double,bmi:double,elective_surgery_double_VectorAssembler_dec361fd3025:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_dec361fd3025:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_dec361fd3025:double,arf_apache_double_VectorAssembler_dec361fd3025:double,gcs_eyes_apache_double_VectorAssembler_dec361fd3025:double,gcs_motor_apache_double_VectorAssembler_dec361fd3025:double,gcs_unable_apache_double_VectorAssembler_dec361fd3025:double,gcs_verbal_apache_double_VectorAssembler_dec361fd3025:double,heart_rate_apache_double_VectorAssembler_dec361fd3025:double,intubated_apache_double_VectorAssembler_dec361fd3025:double,map_apache_double_VectorAssembler_dec361fd3025:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_dec361fd3025:double,d1_diasbp_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_min_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_heartrate_max_double_VectorAssembler_dec361fd3025:double,d1_heartrate_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_min_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_resprate_max_double_VectorAssembler_dec361fd3025:double,d1_resprate_min_double_VectorAssembler_dec361fd3025:double,d1_spo2_max_double_VectorAssembler_dec361fd3025:double,d1_spo2_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_min_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_min_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_diasbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_heartrate_max_double_VectorAssembler_dec361fd3025:double,h1_heartrate_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_min_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_mbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,h1_resprate_max_double_VectorAssembler_dec361fd3025:double,h1_resprate_min_double_VectorAssembler_dec361fd3025:double,h1_spo2_max_double_VectorAssembler_dec361fd3025:double,h1_spo2_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_min_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_max_double_VectorAssembler_dec361fd3025:double,h1_sysbp_noninvasive_min_double_VectorAssembler_dec361fd3025:double,d1_glucose_max_double_VectorAssembler_dec361fd3025:double,d1_glucose_min_double_VectorAssembler_dec361fd3025:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_dec361fd3025:double,cirrhosis_double_VectorAssembler_dec361fd3025:double,diabetes_mellitus_double_VectorAssembler_dec361fd3025:double,hepatic_failure_double_VectorAssembler_dec361fd3025:double,immunosuppression_double_VectorAssembler_dec361fd3025:double,leukemia_double_VectorAssembler_dec361fd3025:double,lymphoma_double_VectorAssembler_dec361fd3025:double,solid_tumor_with_metastasis_double_VectorAssembler_dec361fd3025:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:150)\n\tat org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.<init>(ObjectAggregationIterator.scala:77)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2(ObjectHashAggregateExec.scala:107)\n\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.$anonfun$doExecute$2$adapted(ObjectHashAggregateExec.scala:85)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "#feature_columns = df_2.select([c for c in df_2.columns if c not in {'hospital_death'}])\n",
    "#feature_columns.take(1)\n",
    "\n",
    "va = VectorAssembler(inputCols=[c for c in df_2.columns if c not in {'hospital_death'}], outputCol=\"features\")  \n",
    "output = va.transform(df_2)\n",
    "scaler = StandardScaler(\n",
    "    inputCol = 'features', \n",
    "    outputCol = 'scaledFeatures',\n",
    "    withMean = True,\n",
    "    withStd = True\n",
    ").fit(output)\n",
    "\n",
    "# when we transform the dataframe, the old\n",
    "# feature will still remain in it\n",
    "df_scaled = scaler.transform(output)\n",
    "df_scaled.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting data by type of data for EDA purposes\n",
    "#integer_data = data.select([data.dtypes[i][0] for i in range(len(data.dtypes)) if (data.dtypes[i][1] == 'int') and (data.select(data[i]).distinct().count() >= 10)])\n",
    "\n",
    "#for i in integer_data.columns:\n",
    "#    integer_data.describe(i).show()\n",
    "#    integer_data.select(F.percentile_approx(i, [0.25, 0.75], 100000).alias(\"quantiles\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hospital_deaths = data.select(\"hospital_death\").filter(col(\"hospital_death\")==1 | col(\"hospital_death\")).groupBy(\"hospital_death\").count()\n",
    "# hospital_deaths = hospital_deaths.withColumn('percent', )\n",
    "# hospital_deaths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.hospital_deaths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.select(\"hospital_death\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1715.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 79.0 failed 1 times, most recent failure: Lost task 0.0 in stage 79.0 (TID 79) (udc-ba34-13c0 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_6631368487ec:double,bmi:double,elective_surgery_double_VectorAssembler_6631368487ec:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_6631368487ec:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_6631368487ec:double,arf_apache_double_VectorAssembler_6631368487ec:double,gcs_eyes_apache_double_VectorAssembler_6631368487ec:double,gcs_motor_apache_double_VectorAssembler_6631368487ec:double,gcs_unable_apache_double_VectorAssembler_6631368487ec:double,gcs_verbal_apache_double_VectorAssembler_6631368487ec:double,heart_rate_apache_double_VectorAssembler_6631368487ec:double,intubated_apache_double_VectorAssembler_6631368487ec:double,map_apache_double_VectorAssembler_6631368487ec:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_6631368487ec:double,d1_diasbp_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_min_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_heartrate_max_double_VectorAssembler_6631368487ec:double,d1_heartrate_min_double_VectorAssembler_6631368487ec:double,d1_mbp_max_double_VectorAssembler_6631368487ec:double,d1_mbp_min_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_resprate_max_double_VectorAssembler_6631368487ec:double,d1_resprate_min_double_VectorAssembler_6631368487ec:double,d1_spo2_max_double_VectorAssembler_6631368487ec:double,d1_spo2_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_min_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_heartrate_max_double_VectorAssembler_6631368487ec:double,h1_heartrate_min_double_VectorAssembler_6631368487ec:double,h1_mbp_max_double_VectorAssembler_6631368487ec:double,h1_mbp_min_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_resprate_max_double_VectorAssembler_6631368487ec:double,h1_resprate_min_double_VectorAssembler_6631368487ec:double,h1_spo2_max_double_VectorAssembler_6631368487ec:double,h1_spo2_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_glucose_max_double_VectorAssembler_6631368487ec:double,d1_glucose_min_double_VectorAssembler_6631368487ec:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_6631368487ec:double,cirrhosis_double_VectorAssembler_6631368487ec:double,diabetes_mellitus_double_VectorAssembler_6631368487ec:double,hepatic_failure_double_VectorAssembler_6631368487ec:double,immunosuppression_double_VectorAssembler_6631368487ec:double,leukemia_double_VectorAssembler_6631368487ec:double,lymphoma_double_VectorAssembler_6631368487ec:double,solid_tumor_with_metastasis_double_VectorAssembler_6631368487ec:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:436)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:479)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_6631368487ec:double,bmi:double,elective_surgery_double_VectorAssembler_6631368487ec:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_6631368487ec:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_6631368487ec:double,arf_apache_double_VectorAssembler_6631368487ec:double,gcs_eyes_apache_double_VectorAssembler_6631368487ec:double,gcs_motor_apache_double_VectorAssembler_6631368487ec:double,gcs_unable_apache_double_VectorAssembler_6631368487ec:double,gcs_verbal_apache_double_VectorAssembler_6631368487ec:double,heart_rate_apache_double_VectorAssembler_6631368487ec:double,intubated_apache_double_VectorAssembler_6631368487ec:double,map_apache_double_VectorAssembler_6631368487ec:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_6631368487ec:double,d1_diasbp_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_min_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_heartrate_max_double_VectorAssembler_6631368487ec:double,d1_heartrate_min_double_VectorAssembler_6631368487ec:double,d1_mbp_max_double_VectorAssembler_6631368487ec:double,d1_mbp_min_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_resprate_max_double_VectorAssembler_6631368487ec:double,d1_resprate_min_double_VectorAssembler_6631368487ec:double,d1_spo2_max_double_VectorAssembler_6631368487ec:double,d1_spo2_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_min_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_heartrate_max_double_VectorAssembler_6631368487ec:double,h1_heartrate_min_double_VectorAssembler_6631368487ec:double,h1_mbp_max_double_VectorAssembler_6631368487ec:double,h1_mbp_min_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_resprate_max_double_VectorAssembler_6631368487ec:double,h1_resprate_min_double_VectorAssembler_6631368487ec:double,h1_spo2_max_double_VectorAssembler_6631368487ec:double,h1_spo2_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_glucose_max_double_VectorAssembler_6631368487ec:double,d1_glucose_min_double_VectorAssembler_6631368487ec:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_6631368487ec:double,cirrhosis_double_VectorAssembler_6631368487ec:double,diabetes_mellitus_double_VectorAssembler_6631368487ec:double,hepatic_failure_double_VectorAssembler_6631368487ec:double,immunosuppression_double_VectorAssembler_6631368487ec:double,leukemia_double_VectorAssembler_6631368487ec:double,lymphoma_double_VectorAssembler_6631368487ec:double,solid_tumor_with_metastasis_double_VectorAssembler_6631368487ec:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-b8aa32bb69e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# PCA using 4 components\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pcaFeatures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# extract the transformed features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1715.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 79.0 failed 1 times, most recent failure: Lost task 0.0 in stage 79.0 (TID 79) (udc-ba34-13c0 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_6631368487ec:double,bmi:double,elective_surgery_double_VectorAssembler_6631368487ec:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_6631368487ec:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_6631368487ec:double,arf_apache_double_VectorAssembler_6631368487ec:double,gcs_eyes_apache_double_VectorAssembler_6631368487ec:double,gcs_motor_apache_double_VectorAssembler_6631368487ec:double,gcs_unable_apache_double_VectorAssembler_6631368487ec:double,gcs_verbal_apache_double_VectorAssembler_6631368487ec:double,heart_rate_apache_double_VectorAssembler_6631368487ec:double,intubated_apache_double_VectorAssembler_6631368487ec:double,map_apache_double_VectorAssembler_6631368487ec:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_6631368487ec:double,d1_diasbp_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_min_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_heartrate_max_double_VectorAssembler_6631368487ec:double,d1_heartrate_min_double_VectorAssembler_6631368487ec:double,d1_mbp_max_double_VectorAssembler_6631368487ec:double,d1_mbp_min_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_resprate_max_double_VectorAssembler_6631368487ec:double,d1_resprate_min_double_VectorAssembler_6631368487ec:double,d1_spo2_max_double_VectorAssembler_6631368487ec:double,d1_spo2_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_min_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_heartrate_max_double_VectorAssembler_6631368487ec:double,h1_heartrate_min_double_VectorAssembler_6631368487ec:double,h1_mbp_max_double_VectorAssembler_6631368487ec:double,h1_mbp_min_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_resprate_max_double_VectorAssembler_6631368487ec:double,h1_resprate_min_double_VectorAssembler_6631368487ec:double,h1_spo2_max_double_VectorAssembler_6631368487ec:double,h1_spo2_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_glucose_max_double_VectorAssembler_6631368487ec:double,d1_glucose_min_double_VectorAssembler_6631368487ec:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_6631368487ec:double,cirrhosis_double_VectorAssembler_6631368487ec:double,diabetes_mellitus_double_VectorAssembler_6631368487ec:double,hepatic_failure_double_VectorAssembler_6631368487ec:double,immunosuppression_double_VectorAssembler_6631368487ec:double,leukemia_double_VectorAssembler_6631368487ec:double,lymphoma_double_VectorAssembler_6631368487ec:double,solid_tumor_with_metastasis_double_VectorAssembler_6631368487ec:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2291)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1183)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1177)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.mllib.stat.Statistics$.colStats(Statistics.scala:58)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:436)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:479)\n\tat org.apache.spark.mllib.feature.PCA.fit(PCA.scala:65)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:93)\n\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(VectorAssembler$$Lambda$3174/0x000000084130b840: (struct<age_double_VectorAssembler_6631368487ec:double,bmi:double,elective_surgery_double_VectorAssembler_6631368487ec:double,height:double,pre_icu_los_days:double,weight:double,apache_2_diagnosis_double_VectorAssembler_6631368487ec:double,apache_3j_diagnosis:double,apache_post_operative_double_VectorAssembler_6631368487ec:double,arf_apache_double_VectorAssembler_6631368487ec:double,gcs_eyes_apache_double_VectorAssembler_6631368487ec:double,gcs_motor_apache_double_VectorAssembler_6631368487ec:double,gcs_unable_apache_double_VectorAssembler_6631368487ec:double,gcs_verbal_apache_double_VectorAssembler_6631368487ec:double,heart_rate_apache_double_VectorAssembler_6631368487ec:double,intubated_apache_double_VectorAssembler_6631368487ec:double,map_apache_double_VectorAssembler_6631368487ec:double,resprate_apache:double,temp_apache:double,ventilated_apache_double_VectorAssembler_6631368487ec:double,d1_diasbp_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_min_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_heartrate_max_double_VectorAssembler_6631368487ec:double,d1_heartrate_min_double_VectorAssembler_6631368487ec:double,d1_mbp_max_double_VectorAssembler_6631368487ec:double,d1_mbp_min_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_resprate_max_double_VectorAssembler_6631368487ec:double,d1_resprate_min_double_VectorAssembler_6631368487ec:double,d1_spo2_max_double_VectorAssembler_6631368487ec:double,d1_spo2_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_min_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,d1_sysbp_noninvasive_min:double,d1_temp_max:double,d1_temp_min:double,h1_diasbp_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_min_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_diasbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_heartrate_max_double_VectorAssembler_6631368487ec:double,h1_heartrate_min_double_VectorAssembler_6631368487ec:double,h1_mbp_max_double_VectorAssembler_6631368487ec:double,h1_mbp_min_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_mbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,h1_resprate_max_double_VectorAssembler_6631368487ec:double,h1_resprate_min_double_VectorAssembler_6631368487ec:double,h1_spo2_max_double_VectorAssembler_6631368487ec:double,h1_spo2_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_min_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_max_double_VectorAssembler_6631368487ec:double,h1_sysbp_noninvasive_min_double_VectorAssembler_6631368487ec:double,d1_glucose_max_double_VectorAssembler_6631368487ec:double,d1_glucose_min_double_VectorAssembler_6631368487ec:double,d1_potassium_max:double,d1_potassium_min:double,apache_4a_hospital_death_prob:double,apache_4a_icu_death_prob:double,aids_double_VectorAssembler_6631368487ec:double,cirrhosis_double_VectorAssembler_6631368487ec:double,diabetes_mellitus_double_VectorAssembler_6631368487ec:double,hepatic_failure_double_VectorAssembler_6631368487ec:double,immunosuppression_double_VectorAssembler_6631368487ec:double,leukemia_double_VectorAssembler_6631368487ec:double,lymphoma_double_VectorAssembler_6631368487ec:double,solid_tumor_with_metastasis_double_VectorAssembler_6631368487ec:double,ethnicityIndex:double,genderIndex:double,icu_admit_sourceIndex:double,icu_typeIndex:double,apache_3j_bodysystemIndex:double,apache_2_bodysystemIndex:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:219)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:219)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1429)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$3(RDD.scala:1230)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$5(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\n\t... 31 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# PCA using 4 components\n",
    "pca = PCA(k=4, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
    "model = pca.fit(output)\n",
    "\n",
    "# extract the transformed features\n",
    "result = model.transform(output)\n",
    "\n",
    "result.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
